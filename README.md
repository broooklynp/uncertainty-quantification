# Quarter 1 Project Checkpoint

## SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models

So far this quarter, I have been working with the code that goes along with this [paper](https://arxiv.org/abs/2403.02509) and testing it on different datasets. This repo contains the original code from the paper along with modifications that I made so that the code works with different datasets.

## To run the code:

- Download dependencies from requirements.txt
- Download datasets [SQuAD2.0](https://huggingface.co/datasets/rajpurkar/squad_v2), [TruthfulQA](https://huggingface.co/datasets/domenicrosati/TruthfulQA), and [TriviaQA](https://huggingface.co/datasets/mandarjoshi/trivia_qa)
- Run run.py (located in spuq_code folder) to see results
